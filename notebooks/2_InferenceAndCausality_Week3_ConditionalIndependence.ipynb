{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7871310d",
   "metadata": {},
   "source": [
    "# Conditional Independence\n",
    "**Hands‑on Notebook**\n",
    "\n",
    "\n",
    "**In this notebook**\n",
    "Explore **conditional independence** in chain / fork / collider.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a036c571",
   "metadata": {},
   "source": [
    "## 0) Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2ca1293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "rng = np.random.default_rng(7)\n",
    "\n",
    "def summarize_binary(y, x=None, do=None, df=None, name=\"\"):\n",
    "    if df is not None and x is not None:\n",
    "        p = df.loc[df[x]==1, y].mean()\n",
    "        n = (df[x]==1).sum()\n",
    "        print(f\"P({y}=1 | {x}=1) = {p:.3f}  [n={n}]  {name}\")\n",
    "    if df is not None and do is not None:\n",
    "        p = df[y].mean()\n",
    "        n = len(df)\n",
    "        print(f\"P({y}=1 | do({do})) = {p:.3f}  [n={n}]  {name}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6951d623",
   "metadata": {},
   "source": [
    "\n",
    "## A) Seeing vs Doing with the **Firing Squad** toy model\n",
    "\n",
    "![Firing squad DAG](../images/firing_squad.png)\n",
    "\n",
    "- Variables: \n",
    "`C` (captain order) \n",
    "`SA` (squad A fires)\n",
    "`SB` (squad B fires)\n",
    "`D` (death).  \n",
    "\n",
    "\n",
    "We will compare:\n",
    "- **Observation**: `P(D|SA=0)` — low, because when A doesn't fires, usually B also does not fire (same cause `C`).\n",
    "- **Intervention**: `P(D|do(SA=0))` — set A to not fire regardless of the command `C`; isolate A's own causal contribution. Now we have some times B firind and some times B not firing resulting in a more complex scenario.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85456374",
   "metadata": {},
   "source": [
    "To have a bit of more interesting scenario, we impose some imperfection to obediance of our squads in the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f3cbb40c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Observational world:\n",
      "P(D=1 | SA=0) = 1.000  [n=52415]  (seeing)\n",
      "\n",
      "Interventional world:\n",
      "P(D=1 | do(SA=0)) = 0.475  [n=100000]  (doing)\n"
     ]
    }
   ],
   "source": [
    "N = 100000\n",
    "rng = np.random.default_rng(6)\n",
    "\n",
    "# Structural equations (binary)\n",
    "C = rng.binomial(1, 0.5, size=N)  # Captain order\n",
    "SA = (C & (rng.random(N) < 0.95)).astype(int)  # Squad A obeys if ordered\n",
    "SB = (C & (rng.random(N) < 0.95)).astype(int)  # Squad B obeys if ordered\n",
    "\n",
    "# Lethality (set to 1 for simplicity, adjust if you want realism)\n",
    "p_kill_A, p_kill_B = 1.0, 1.0\n",
    "hit_A = p_kill_A\n",
    "hit_B = p_kill_B\n",
    "# hit_A = (SA & (rng.random(N) < p_kill_A)).astype(int)\n",
    "# hit_B = (SB & (rng.random(N) < p_kill_B)).astype(int)\n",
    "D = np.maximum(hit_A, hit_B)\n",
    "\n",
    "# Combine all simulated variables into a single DataFrame for easier analysis and plotting\n",
    "obs_df = pd.DataFrame(dict(C=C, SA=SA, SB=SB, D=D))\n",
    "\n",
    "print(\"Observational world:\")\n",
    "# Compute P(D=1 | SA=0)\n",
    "p_obs = obs_df.loc[obs_df[\"SA\"] == 0, \"D\"].mean()\n",
    "n_obs = (obs_df[\"SA\"] == 0).sum()\n",
    "print(f\"P(D=1 | SA=0) = {p_obs:.3f}  [n={n_obs}]  (seeing)\")\n",
    "\n",
    "# --- Interventional: do(SA=0) ---\n",
    "C2 = rng.binomial(1, 0.5, size=N)                   # captain as before\n",
    "SA2 = np.zeros(N, dtype=int)                        # force A not to fire\n",
    "SB2 = (C2 & (rng.random(N) < 0.95)).astype(int)     # B still reacts to captain\n",
    "\n",
    "hit_A2 = (SA2 & (rng.random(N) < p_kill_A)).astype(int)\n",
    "hit_B2 = (SB2 & (rng.random(N) < p_kill_B)).astype(int)\n",
    "D2 = np.maximum(hit_A2, hit_B2)\n",
    "\n",
    "do_df = pd.DataFrame(dict(C=C2, SA=SA2, SB=SB2, D=D2))\n",
    "p_do = do_df[\"D\"].mean()\n",
    "n_do = len(do_df)\n",
    "\n",
    "print(\"\\nInterventional world:\")\n",
    "print(f\"P(D=1 | do(SA=0)) = {p_do:.3f}  [n={n_do}]  (doing)\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44c84796",
   "metadata": {},
   "source": [
    "Number n above shows the number of tests where SA=0 happened. When we only observed, about half of the time, SA=0 and when we intervened, it was always kept at 0."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a4d4d9",
   "metadata": {},
   "source": [
    "#### We see that P(D=1 | SA=0) ≠ P(D=1 | do(SA=0))!\n",
    "This difference reveals that **C (the captain’s order)** is a **confounder** — it influences both the squad’s action (`SA`) and the outcome (`D`).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ab55b5",
   "metadata": {},
   "source": [
    "## B) Conditional Independence in **Chain / Fork / Collider**\n",
    "\n",
    "In this section we simulate three fundamental causal structures (often called the *building blocks* of causal graphs)  \n",
    "to explore how **conditional independence** behaves in each.\n",
    "\n",
    "Reminder:\n",
    "### Marginal vs Conditional Correlation\n",
    "\n",
    "- **Marginal correlation** measures how two variables vary together *overall*, without taking any other variables into account.  \n",
    "  → Example: the raw relationship between Smoking and Cancer in the population.\n",
    "\n",
    "- **Conditional correlation** measures how two variables relate *after we fix or control for* a third variable.  \n",
    "  → Example: the relationship between Smoking and Cancer **within each level of Tar exposure**.\n",
    "\n",
    "**Key idea:**  \n",
    "If two variables are correlated marginally but not conditionally, it means a third variable (a mediator or confounder) explains their association.  \n",
    "Conversely, if they are independent marginally but correlated conditionally, conditioning has **opened a path** (as in collider bias).\n",
    "\n",
    "---\n",
    "\n",
    "### 1. Chain: A → B → C\n",
    "**Interpretation:**  \n",
    "B, \"the mediator\" transmits information or influence from A to C.  \n",
    "- *Example:* Smoking → Tar in lungs → Cancer.  \n",
    "- A and C are correlated because information “flows” through B.  \n",
    "- **If we condition on B**, we block that path — A and C become (approximately) independent.\n",
    "\n",
    "**Expectation:**  \n",
    "- Marginal correlation: high (A and C move together).  \n",
    "- Conditional correlation given B: ≈ 0 (path blocked).\n",
    "\n",
    "---\n",
    "\n",
    "### 2. Fork (Confounding): A ← U → C\n",
    "**Interpretation:**  \n",
    "U is a *common cause* (confounder) of both A and C.  \n",
    "- *Example:* Genetic predisposition → Smoking and Cancer.  \n",
    "- A and C appear correlated, but only because of U.  \n",
    "- **If we condition on U**, we remove that shared cause and eliminate the spurious correlation.\n",
    "\n",
    "**Expectation:**  \n",
    "- Marginal correlation: high (U induces a false link).  \n",
    "- Conditional correlation given U: ≈ 0 (confounding removed).\n",
    "\n",
    "---\n",
    "\n",
    "### 3. Collider: A → B ← C\n",
    "**Interpretation:**  \n",
    "B is a *common effect* (collider) of A and C.  \n",
    "- *Example:*  \n",
    "  - A = Smoking  \n",
    "  - C = Air pollution  \n",
    "  - B = Hospital admission (caused by either).  \n",
    "- Normally, A and C are independent.  \n",
    "- **If we condition on B** (or any descendant of B), we *create* a correlation between A and C —  \n",
    "  this is known as **collider bias** or **selection bias**.\n",
    "\n",
    "**Expectation:**  \n",
    "- Marginal correlation: near 0 (A, C independent).  \n",
    "- Conditional correlation given B: strong (conditioning opens the path).\n",
    "\n",
    "---\n",
    "\n",
    "\n",
    "### Conclusion\n",
    "> Correlation alone can mislead: depending on the graph, conditioning can remove, reveal, or even **fabricate** relationships.\n",
    ">\n",
    "> Understanding which paths are open or closed (via **d-separation**) is central to causal inference.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7349a779",
   "metadata": {},
   "source": [
    "\n",
    "### What the following block of code does\n",
    "- Each function (`sim_chain`, `sim_fork`, `sim_collider`) simulates random data following these causal relationships.  \n",
    "- We compute:\n",
    "  - **Marginal correlation**: `corr(A, C)`  \n",
    "  - **Conditional correlation**: `corr(A, C | middle node)` using simple binning on the conditioning variable.\n",
    "- This shows how *conditioning* can either **block** or **create** associations depending on the graph structure.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "069adc8b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chain: corr(A,C)  (marginal) = 0.5801684286913067\n",
      "Fork:  corr(A,C)  (marginal) = 0.4971395078854403\n",
      "Collider: corr(A,C) (marginal) = -0.010648989688450418\n",
      "\n",
      "Conditioning (approx via binning):\n",
      "Chain: corr(A,C | B) ≈ 0.04719708880720996\n",
      "Fork:  corr(A,C | U) ≈ 0.03878851215357193\n",
      "Collider: corr(A,C | B) ≈ -0.4745972035238439\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def sim_chain(N=50_000, seed=1):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    A = rng.normal(0,1,N)\n",
    "    B = A + rng.normal(0,1,N)\n",
    "    C = B + rng.normal(0,1,N)\n",
    "    return pd.DataFrame(dict(A=A,B=B,C=C))\n",
    "\n",
    "def sim_fork(N=50_000, seed=2):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    U = rng.normal(0,1,N)\n",
    "    A = U + rng.normal(0,1,N)\n",
    "    C = U + rng.normal(0,1,N)\n",
    "    return pd.DataFrame(dict(U=U,A=A,C=C))\n",
    "\n",
    "def sim_collider(N=50_000, seed=3):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    A = rng.normal(0,1,N)\n",
    "    C = rng.normal(0,1,N)\n",
    "    B = A + C + rng.normal(0,1,N)  # collider\n",
    "    return pd.DataFrame(dict(A=A,B=B,C=C))\n",
    "\n",
    "def corr(x,y,df):\n",
    "    return np.corrcoef(df[x], df[y])[0,1]\n",
    "\n",
    "chain = sim_chain()\n",
    "fork = sim_fork()\n",
    "coll = sim_collider()\n",
    "\n",
    "print(\"Chain: corr(A,C)  (marginal) =\", corr(\"A\",\"C\", chain))\n",
    "print(\"Fork:  corr(A,C)  (marginal) =\", corr(\"A\",\"C\", fork))\n",
    "print(\"Collider: corr(A,C) (marginal) =\", corr(\"A\",\"C\", coll))\n",
    "\n",
    "# Conditioning effects\n",
    "def partial_corr_xy_given_z(x,y,z,df, bins=10):\n",
    "    # Approximate partial correlation by binning on z (simple classroom-friendly approach).\n",
    "    df2 = df.copy()\n",
    "    df2[\"_zb\"] = pd.qcut(df2[z], q=bins, duplicates=\"drop\")\n",
    "    vals = []\n",
    "    for _,grp in df2.groupby(\"_zb\", observed=True):\n",
    "        if len(grp)>5:\n",
    "            vals.append(np.corrcoef(grp[x], grp[y])[0,1])\n",
    "    return np.nanmean(vals)\n",
    "\n",
    "print(\"\\nConditioning (approx via binning):\")\n",
    "print(\"Chain: corr(A,C | B) ≈\", partial_corr_xy_given_z(\"A\",\"C\",\"B\", chain))\n",
    "print(\"Fork:  corr(A,C | U) ≈\", partial_corr_xy_given_z(\"A\",\"C\",\"U\", fork))\n",
    "print(\"Collider: corr(A,C | B) ≈\", partial_corr_xy_given_z(\"A\",\"C\",\"B\", coll))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac85f572",
   "metadata": {},
   "source": [
    "### Interpretation of Results\n",
    "\n",
    "| Structure | Marginal Corr(A, C) | Conditional Corr(A, C \\| Z) | What it shows |\n",
    "|------------|--------------------:|-----------------------------:|----------------|\n",
    "| **Chain** | 0.58 | 0.05 | Conditioning on the mediator **B** blocks the flow from A → B → C. |\n",
    "| **Fork** | 0.50 | 0.04 | Conditioning on the confounder **U** removes the common-cause association. |\n",
    "| **Collider** | −0.01 | −0.47 | Conditioning on **B** (a common effect) creates a spurious link — classic *collider bias*. |\n",
    "\n",
    "**Summary:**  \n",
    "- **Chain & Fork:** conditioning *reduces* correlation (closes the path).  \n",
    "- **Collider:** conditioning *induces* correlation (opens a blocked path).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e1b3183",
   "metadata": {},
   "source": [
    "\n",
    "## C) **Proxy variable** for an unobserved confounder\n",
    "\n",
    "Unobserved `U` (true smoking exposure) affects both `YellowTeeth (Z)` and `Cancer (Y)`;  \n",
    "`Smoking (X)` is noisy self-report we can't rely on for percision issues. Nicotin level in body measured accurately `NL` serves as a **proxy** for `U`.\n",
    "\n",
    "We compare naive estimate `P(Y|X)` with adjustment by the proxy `NL` (back-door via proxy).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "446d4a61",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicting Y (cancer) using self reported smoking only\n",
      "Naive (Y ~ X):\n",
      "  beta_X = 0.579\n",
      "\n",
      "Proxy-adjusted to include both self report and Nicotin level biomarker(Y ~ X + NL):\n",
      "  beta_X  = 0.006   (should shrink toward 0)\n",
      "  beta_NL = 1.356  (captures the true U effect)\n",
      "\n",
      "Predicting Y (cancer) using Biomarker only (Y ~ NL):\n",
      "  beta_NL = 1.361\n"
     ]
    }
   ],
   "source": [
    "# --- Section C (revised): Proxy variable with accurate biomarker NL ---\n",
    "\n",
    "N = 200_000\n",
    "rng = np.random.default_rng(12)\n",
    "\n",
    "# Unobserved true exposure\n",
    "U = rng.normal(0, 1, N)                 # unobserved driver of risk\n",
    "\n",
    "# Observed variables\n",
    "X  = U + rng.normal(0, 1.0, N)          # self-report (noisy, low precision)\n",
    "Z  = U + rng.normal(0, 0.8, N)          # yellow teeth (crude indicator; we won't use it for adjustment here)\n",
    "NL = U + rng.normal(0, 0.1, N)          # biomarker (accurate proxy; low noise)\n",
    "\n",
    "# Outcome depends on TRUE exposure (U), not X directly\n",
    "logit = -0.7 + 1.4 * U\n",
    "pY = 1 / (1 + np.exp(-logit))\n",
    "Y = (rng.random(N) < pY).astype(int)\n",
    "\n",
    "dfp = pd.DataFrame(dict(X=X, Z=Z, NL=NL, Y=Y))\n",
    "\n",
    "# --- Models: naive vs proxy-adjusted ---\n",
    "import statsmodels.api as sm\n",
    "\n",
    "# 1) Naive: Y ~ X  (confounded by U)\n",
    "m_naive = sm.Logit(dfp[\"Y\"], sm.add_constant(dfp[[\"X\"]])).fit(disp=False)\n",
    "\n",
    "# 2) Proxy-adjusted with accurate biomarker: Y ~ X + NL\n",
    "m_proxy = sm.Logit(dfp[\"Y\"], sm.add_constant(dfp[[\"X\",\"NL\"]])).fit(disp=False)\n",
    "\n",
    "# 3) Biomarker only: Y ~ NL  (close to the \"oracle\" using U)\n",
    "m_biomarker = sm.Logit(dfp[\"Y\"], sm.add_constant(dfp[[\"NL\"]])).fit(disp=False)\n",
    "\n",
    "print(\"Predicting Y (cancer) using self reported smoking only\")\n",
    "print(\"Naive (Y ~ X):\")\n",
    "print(f\"  beta_X = {m_naive.params['X']:.3f}\")\n",
    "\n",
    "print(\"\\nProxy-adjusted to include both self report and Nicotin level biomarker(Y ~ X + NL):\")\n",
    "print(f\"  beta_X  = {m_proxy.params['X']:.3f}   (should shrink toward 0)\")\n",
    "print(f\"  beta_NL = {m_proxy.params['NL']:.3f}  (captures the true U effect)\")\n",
    "\n",
    "print(\"\\nPredicting Y (cancer) using Biomarker only (Y ~ NL):\")\n",
    "print(f\"  beta_NL = {m_biomarker.params['NL']:.3f}\")\n",
    "\n",
    "# (Optional instructor check — uncomment to peek at \"truth\")\n",
    "# corr_U_X  = np.corrcoef(U, X)[0,1]\n",
    "# corr_U_Z  = np.corrcoef(U, Z)[0,1]\n",
    "# corr_U_NL = np.corrcoef(U, NL)[0,1]\n",
    "# print(f\"\\n[Hidden truth] corr(U,X)={corr_U_X:.2f}, corr(U,Z)={corr_U_Z:.2f}, corr(U,NL)={corr_U_NL:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "049a5454",
   "metadata": {},
   "source": [
    "\n",
    "> **Observation:** With only `X` we pick up confounding from `U`.  \n",
    "> Adding the proxy `Z` absorbs much of `U`'s influence and moves `beta_X` toward the *direct* effect.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdc3d31b",
   "metadata": {},
   "source": [
    "## Excersice:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c1cfad8",
   "metadata": {},
   "source": [
    "Secion A) In the parameterization `P(D|SA=0)` and `P(D|do(SA=0))` can both be close to 1.  \n",
    "What *qualitatively* changes between the two worlds? Explain using a one-sentence reference to the DAG.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2d2e41",
   "metadata": {},
   "source": [
    "\n",
    "## E) Quick Tasks (for credit / discussion)\n",
    "\n",
    "1. **Parameter flip:** In the firing squad model, change `p_kill_A` to 0.6 and `p_kill_B` to 0.99.  \n",
    "   - Re-run and record `P(D|SA=1)` vs `P(D|do(SA=1))`.  \n",
    "   - Explain in one sentence which way confounding moves the observational estimate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2852a742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "P(D=1 | SA=1) = 0.977  [n=47585]  (seeing)\n",
      "P(D=1 | do(SA=1)) = 0.788  [n=100000]  (doing)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "N = 100_000\n",
    "rng = np.random.default_rng(6)\n",
    "\n",
    "# Observational\n",
    "C = rng.binomial(1, 0.5, N)\n",
    "SA = (C & (rng.random(N) < 0.95)).astype(int)\n",
    "SB = (C & (rng.random(N) < 0.95)).astype(int)\n",
    "p_kill_A, p_kill_B = 0.6, 0.99\n",
    "hit_A = (SA & (rng.random(N) < p_kill_A)).astype(int)\n",
    "hit_B = (SB & (rng.random(N) < p_kill_B)).astype(int)\n",
    "D = np.maximum(hit_A, hit_B)\n",
    "p_obs = pd.DataFrame({'SA':SA, 'D':D}).loc[lambda df: df['SA']==1, 'D'].mean()\n",
    "n_obs = (SA==1).sum()\n",
    "print(f\"P(D=1 | SA=1) = {p_obs:.3f}  [n={n_obs}]  (seeing)\")\n",
    "\n",
    "# Interventional do(SA=1)\n",
    "C2 = rng.binomial(1, 0.5, N)\n",
    "SA2 = np.ones(N, dtype=int)\n",
    "SB2 = (C2 & (rng.random(N) < 0.95)).astype(int)\n",
    "hit_A2 = (SA2 & (rng.random(N) < p_kill_A)).astype(int)\n",
    "hit_B2 = (SB2 & (rng.random(N) < p_kill_B)).astype(int)\n",
    "D2 = np.maximum(hit_A2, hit_B2)\n",
    "p_do = D2.mean()\n",
    "print(f\"P(D=1 | do(SA=1)) = {p_do:.3f}  [n={N}]  (doing)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8273901a",
   "metadata": {},
   "source": [
    "Q: E1-2: Confounding biases the observational estimate upward because SA=1 usually co-occurs with C=1, which also triggers SB to fire with ~99% lethality."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ab763f8",
   "metadata": {},
   "source": [
    "2. **Collider bias:** In section B, filter to the top 10% of `B` values in the collider model and compute `corr(A,C)` there.  \n",
    "   - Why does this selection amplify the association?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "e0e253af",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "corr(A,C | B in top 10%) = -0.377\n"
     ]
    }
   ],
   "source": [
    "def sim_collider(N=50_000, seed=3):\n",
    "    rng = np.random.default_rng(seed)\n",
    "    A = rng.normal(0,1,N); C = rng.normal(0,1,N)\n",
    "    B = A + C + rng.normal(0,1,N)\n",
    "    return pd.DataFrame({'A':A, 'B':B, 'C':C})\n",
    "\n",
    "coll = sim_collider()\n",
    "top10 = coll['B'].quantile(0.9)\n",
    "coll_top = coll[coll['B'] >= top10]\n",
    "corr_top = np.corrcoef(coll_top['A'], coll_top['C'])[0,1]\n",
    "print(f\"corr(A,C | B in top 10%) = {corr_top:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf6bc973",
   "metadata": {},
   "source": [
    "Description: Selecting the top 10% of the collider B conditions on a common effect, opening a spurious path between A and C and inducing a strong negative correlation that did not exist marginally — collider bias."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00d04360",
   "metadata": {},
   "source": [
    "3. Proxy strength: In section C, increase proxy noise (e.g., Z = U + 1.5*eps_z).\n",
    "\n",
    "How do beta_X and beta_Z change? What does this say about weak proxies?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "a3afe401",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive (X):      β_X = 0.579\n",
      "Weak (X+Z):     β_X = 0.486 , β_Z = 0.219\n",
      "Strong (X+NL):  β_X = 0.005 , β_NL = 1.357\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd, statsmodels.api as sm\n",
    "N = 200_000\n",
    "rng = np.random.default_rng(12)\n",
    "\n",
    "U = rng.normal(0,1,N)                       # true confounder\n",
    "X = U + rng.normal(0,1.0,N)                 # noisy self-report\n",
    "NL = U + rng.normal(0,0.1,N)                # strong biomarker\n",
    "Z = U + 1.5 * rng.normal(0,1,N)             # weak proxy (high noise)\n",
    "\n",
    "logit = -0.7 + 1.4 * U\n",
    "Y = (rng.random(N) < 1/(1+np.exp(-logit))).astype(int)\n",
    "df = pd.DataFrame({'X':X, 'Z':Z, 'NL':NL, 'Y':Y})\n",
    "\n",
    "m_naive = sm.Logit(df['Y'], sm.add_constant(df[['X']])).fit(disp=False)\n",
    "m_weak  = sm.Logit(df['Y'], sm.add_constant(df[['X','Z']])).fit(disp=False)\n",
    "m_strong= sm.Logit(df['Y'], sm.add_constant(df[['X','NL']])).fit(disp=False)\n",
    "\n",
    "print(f\"Naive (X):      β_X = {m_naive.params['X']:.3f}\")\n",
    "print(f\"Weak (X+Z):     β_X = {m_weak.params['X']:.3f} , β_Z = {m_weak.params['Z']:.3f}\")\n",
    "print(f\"Strong (X+NL):  β_X = {m_strong.params['X']:.3f} , β_NL = {m_strong.params['NL']:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c7c0e2b",
   "metadata": {},
   "source": [
    "Description: A weak proxy (Z with high noise) captures almost none of the confounding, leaving β_X nearly unchanged and β_Z tiny, demonstrating that weak proxies fail to de-bias the estimate."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29431fd3",
   "metadata": {},
   "source": [
    "4. Back-door bins: In section D, increase the number of U bins from 10 to 30.\n",
    "\n",
    "Does the adjusted estimate stabilize? Why / why not?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "83054f82",
   "metadata": {
    "vscode": {
     "languageId": "markdown"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 bins  → ATE = 0.0513\n",
      "30 bins  → ATE = 0.0513\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "N = 200_000\n",
    "rng = np.random.default_rng(15)\n",
    "\n",
    "U = rng.binomial(1, 0.5, N)                     # binary confounder\n",
    "X = (U + rng.random(N) < 0.8).astype(int)       # treatment\n",
    "Y = (U + 0.7*X + rng.random(N) < 0.9).astype(int)\n",
    "\n",
    "df = pd.DataFrame({'U':U, 'X':X, 'Y':Y})\n",
    "\n",
    "def stratified_ate(bins):\n",
    "    df['_bin'] = pd.qcut(df['U'], q=bins, duplicates='drop')\n",
    "    ate = []\n",
    "    for _, g in df.groupby('_bin', observed=True):\n",
    "        if len(g)<10: continue\n",
    "        p1 = g.loc[g['X']==1, 'Y'].mean()\n",
    "        p0 = g.loc[g['X']==0, 'Y'].mean()\n",
    "        ate.append(p1-p0)\n",
    "    return np.mean(ate)\n",
    "\n",
    "print(f\"10 bins  → ATE = {stratified_ate(10):.4f}\")\n",
    "print(f\"30 bins  → ATE = {stratified_ate(30):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b9cd3de",
   "metadata": {},
   "source": [
    "Description: Increasing bins from 10 to 30 stabilizes the back-door ATE because finer stratification better approximates conditioning on the true confounder U, reducing residual confounding."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
